{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EqFC5bFnTeZx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define model hyperparameters\n",
        "BATCH_SIZE = 32  # Parallel sequences processed\n",
        "CONTEXT_WINDOW = 8  # Max context length for predictions\n",
        "EPOCHS = 5000\n",
        "CHECKPOINT_INTERVAL = 500\n",
        "LR = 3e-4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 200\n",
        "EMBEDDING_DIM = 384\n",
        "HEADS = 6\n",
        "LAYERS = 6\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "torch.manual_seed(457)\n",
        "\n",
        "# Load dataset\n",
        "with open('stoic.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "# Character encoding setup\n",
        "char_list = sorted(set(corpus))\n",
        "VOCAB_SIZE = len(char_list)\n",
        "char_to_index = {ch: i for i, ch in enumerate(char_list)}\n",
        "index_to_char = {i: ch for i, ch in enumerate(char_list)}\n",
        "\n",
        "encode_text = lambda s: [char_to_index[c] for c in s]\n",
        "decode_text = lambda l: ''.join([index_to_char[i] for i in l])\n",
        "\n",
        "# Train-validation split\n",
        "data_tensor = torch.tensor(encode_text(corpus), dtype=torch.long)\n",
        "split_idx = int(0.9 * len(data_tensor)) #we'll be training with the first 90% of the data and do validation with the rest\n",
        "train_data, val_data = data_tensor[:split_idx], data_tensor[split_idx:]\n",
        "\n",
        "\n",
        "# Function to generate mini-batches\n",
        "def get_batch(mode):\n",
        "    dataset = train_data if mode == 'train' else val_data\n",
        "    idxs = torch.randint(len(dataset) - CONTEXT_WINDOW, (BATCH_SIZE,))\n",
        "    x_batch = torch.stack([dataset[i:i + CONTEXT_WINDOW] for i in idxs])\n",
        "    y_batch = torch.stack([dataset[i + 1:i + CONTEXT_WINDOW + 1] for i in idxs])\n",
        "    return x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_loss():\n",
        "    losses = {}\n",
        "    model.eval()\n",
        "    for mode in ['train', 'val']:\n",
        "        batch_losses = torch.zeros(EVAL_ITERS)\n",
        "        for i in range(EVAL_ITERS):\n",
        "            x, y = get_batch(mode)\n",
        "            _, loss = model(x, y)\n",
        "            batch_losses[i] = loss.item()\n",
        "        losses[mode] = batch_losses.mean()\n",
        "    model.train()\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#singkle head of attention\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_dim):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.query = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.value = nn.Linear(EMBEDDING_DIM, head_dim, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW)))\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #here B-> batch, T -> time-step, C -> channels\n",
        "        B, T, C = x.shape\n",
        "        k, q, v = self.key(x), self.query(x), self.value(x) # dimesnsion of this is (B,T,HeadSize)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) * k.shape[-1]**-0.5\n",
        "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        return attention_probs @ v\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, EMBEDDING_DIM)\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, EMBEDDING_DIM):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(EMBEDDING_DIM, 4 * EMBEDDING_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * EMBEDDING_DIM, EMBEDDING_DIM),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#communication followed by computation\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, EMBEDDING_DIM, HEADS):\n",
        "        super().__init__()\n",
        "        head_size = EMBEDDING_DIM // HEADS\n",
        "        self.mha = MultiHeadAttention(HEADS, head_size)\n",
        "        self.ffwd = FeedForward(EMBEDDING_DIM)\n",
        "        self.ln1 = nn.LayerNorm(EMBEDDING_DIM)\n",
        "        self.ln2 = nn.LayerNorm(EMBEDDING_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "hE3K_ybvUgaO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "        self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBEDDING_DIM)\n",
        "        self.blocks = nn.Sequential(*[Block(EMBEDDING_DIM, HEADS=HEADS) for _ in range(LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(EMBEDDING_DIM)\n",
        "        self.lm_head = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -CONTEXT_WINDOW:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = LanguageModel()\n",
        "m = model.to(DEVICE)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# We'll be using the AdamW optimizer for best results\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    if epoch % CHECKPOINT_INTERVAL == 0 or epoch == EPOCHS - 1:\n",
        "        losses = compute_loss()\n",
        "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(decode_text(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eew3eUIKUkZw",
        "outputId": "de73a1fc-7be2-42b6-c3bb-bb190758aef8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.713691 M parameters\n",
            "step 0: train loss 4.4460, val loss 4.4495\n",
            "step 500: train loss 2.1563, val loss 2.2279\n",
            "step 1000: train loss 2.0441, val loss 2.1356\n",
            "step 1500: train loss 1.9811, val loss 2.0880\n",
            "step 2000: train loss 1.9382, val loss 2.0526\n",
            "step 2500: train loss 1.8820, val loss 2.0114\n",
            "step 3000: train loss 1.8584, val loss 1.9786\n",
            "step 3500: train loss 1.8271, val loss 1.9861\n",
            "step 4000: train loss 1.8289, val loss 1.9594\n",
            "step 4500: train loss 1.7941, val loss 1.9273\n",
            "step 4999: train loss 1.7831, val loss 1.9317\n",
            "\n",
            "have‟m? Are man will yhe he good;* what the subber the sorking see as sect not if are is of ke that peopion, then to one do, know Wish these contrantingure not‟re time elsels that gerthing twhigh to may inhored to things wayschd, and lityus\n",
            "of the low\n",
            "your? Aster in it a reper a soulf, ha8*\n",
            "what therad. The elimmb up hen hast if the was diver stiveradly\n",
            "to he this is to unn rehile is pleciad it an thou seerve coun chat that roough by did to she\n",
            "charoed is resurgery to perehaptes which have wass \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('more_stoic.txt', 'w').write(decode_text(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcTzP-sFU4G8",
        "outputId": "5915b870-346a-40eb-cb9b-e44a24e8a715"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode_text(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoOGvSGbbtI6",
        "outputId": "99ec3aa2-2987-4c7a-da85-3121ec53efa1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hy them that the say comnd afteren he heap? To should urate demire the\n",
            "perion I men to what the are other, bundly you abOut mone to gry you are in if tobutt (on the make this man if world with a you awaturelf or charef and all may eatx, and or they worther worde. Of alsef. But in refferenter a coelation: with there colsist the prenestus the such desire, or would that\n",
            "is\n",
            "also to life to they on bean ablamle to\n",
            "to plisicestess teycarly nor withere the growife you from lige to ovherar?\n",
            "which has si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQ0IXJ7Ob_gS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}